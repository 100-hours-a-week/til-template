## 날짜: 2025-03-05

### 스크럼
- 학습 목표 1 : Early Stopping & Regularization
- 학습 목표 2 : Hyperparameter Tuning (GridSearch & RandomSearch)
- 학습 목표 3 : 모델 학습 최적화 방법론

### 새로 배운 내용

#### Early Stopping
- 과적합 방지를 위한 조기 종료 기법 이해 및 적용
- 검증 손실을 모니터링하여 일정 에포크 동안 개선되지 않으면 학습 중단
- 실습을 통해 조기 종료가 모델 성능에 미치는 영향 분석

#### Hyperparameter Tuning
- GridSearch와 RandomSearch를 활용한 최적의 하이퍼파라미터 탐색
- **GridSearch**: 모든 조합을 탐색하여 최적의 값을 찾음
- **RandomSearch**: 일부 조합을 무작위로 탐색하여 자원 소모를 줄이는 방식 적용
- 머신러닝뿐만 아니라 딥러닝에도 적용 가능함을 학습

#### 모델 학습 최적화
- 대규모 모델 학습 시 자원 효율성을 고려한 전략
- Colab GPU를 활용한 학습 방법 및 체크포인트 저장 후 재학습 고려
- 최적의 에포크 수를 찾는 방법 탐색

### 오늘의 도전 과제와 해결 방법
- **도전 과제 1**: 하이퍼파라미터 튜닝 시 연산 비용을 줄이는 방법 탐색
  - **해결 방법**: GridSearch와 RandomSearch의 차이를 분석하고, 상황에 맞는 방법 선택
- **도전 과제 2**: 대규모 모델 학습 시 효율적인 학습 전략 고민
  - **해결 방법**: Colab GPU 사용 시 체크포인트 저장 및 이어서 학습하는 방법 검토

### 오늘의 회고
- GridSearch와 RandomSearch를 머신러닝뿐만 아니라 딥러닝에도 적용 가능하다는 점을 새롭게 깨달음.
- 연산 비용과 자원 소모를 고려하여 신중하게 하이퍼파라미터 튜닝을 진행하는 것이 중요함.
- 프로젝트 진행 시 Colab의 GPU를 활용한 학습 최적화 전략을 고민해봐야겠다고 생각함.
